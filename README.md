# Crowd Density Estimation using U-Net

This repository contains a complete pipeline for crowd density estimation using a U-Net-based convolutional neural network. The system is capable of predicting spatially-aware density maps from RGB images. These predictions can be used to estimate crowd counts and distributions, and are suitable for applications in public safety, surveillance, urban planning, and event management.

## Overview

The core of this project is a U-Net architecture implemented in PyTorch, trained to estimate density maps from images. The training pipeline supports preprocessing, data augmentation, logging, visualization, and evaluation, and can be extended to other segmentation or regression tasks.

## Features

- U-Net model with modular encoder-decoder design
- Weighted Mean Squared Error loss to emphasize high-density regions
- Automated training pipeline with support for multiple configurations
- Visualization tools for density maps and training metrics
- Custom dataset class with preprocessing and augmentation
- Support for real-time monitoring and validation tracking

## Directory Structure

```
.
├── main.py                  # Entry point for training and validation
├── automated_training.py    # Script for running multiple training sessions
├── dataset_class.py         # Dataset handling and density map generation
├── unet.py                  # U-Net model definition
├── unet_components.py       # Building blocks for U-Net layers
├── unet_trainer.py          # Training logic and validation loop
├── utility.py               # Helper functions for I/O and preprocessing
├── visualization.py         # Visualization of training progress and results
├── requirements.txt         # List of dependencies
```

## Dataset Requirements

The project assumes the use of a dataset formatted with image-density map pairs. A typical structure is:

```
dataset/
├── train/
│   ├── images/
│   └── ground_truth/
├── test/
│   ├── images/
│   └── ground_truth/
```

The ground truth density maps are created by placing Gaussian kernels over the annotated head positions of individuals in the images.

## Preprocessing and Augmentation

- All images and maps are resized to 512×512
- Horizontal flipping and color jitter are applied during training
- Density maps are generated by convolving person annotations with a Gaussian kernel

## Model Architecture

The U-Net consists of an encoder-bottleneck-decoder structure with skip connections between corresponding encoder and decoder layers. Each encoder block applies two 3×3 convolutions followed by max pooling, while each decoder block uses transposed convolution for upsampling, concatenation with encoder features, and double convolution for refinement.

### Output

The model outputs a single-channel predicted density map with the same spatial dimensions as the input image.

## Training Configuration

- Loss Function: Weighted Mean Squared Error (MSE)
- Optimizer: Adam with default momentum and adaptive learning rate
- Input channels: 3 (RGB)
- Output channels: 1 (density map)
- Validation after every epoch using standard MSE

## Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/yourusername/crowd-density-estimation.git
cd crowd-density-estimation
pip install -r requirements.txt
```

## Running the Model

To train the model:

```bash
python main.py
```

To run batch experiments with varying parameters:

```bash
python automated_training.py
```

## Visualization

Training progress, loss curves, and predicted density maps can be visualized using the included visualization tools. This is useful for debugging and evaluating model behavior across epochs.

## Future Improvements

- Improve the model’s sensitivity to sparse regions
- Reduce false positives in complex background environments
- Incorporate more advanced data augmentation strategies
- Explore lightweight variants for edge deployment

## License

This project is licensed under the MIT License.
